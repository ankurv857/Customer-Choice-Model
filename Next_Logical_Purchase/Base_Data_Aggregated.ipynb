{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/a0v0022/Documents/Logical_Purchase'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.sparse import dok_matrix, coo_matrix\n",
    "from sklearn.utils.multiclass import  type_of_target\n",
    "import multiprocessing\n",
    "import itertools\n",
    "from sklearn.ensemble import GradientBoostingClassifier as gbm \n",
    "from sklearn.cross_validation import KFold\n",
    "import matplotlib.pylab as plt\n",
    "from datetime import datetime, timedelta\n",
    "import multiprocessing\n",
    "from joblib import Parallel , delayed\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import xgboost as xgb\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Logical_purchase_5500.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.columns = ['user_id','visit_date','product_id','units','dollars','cust_lead_date','cust_prod_lead_date','order_id','reordered','days_since_prior_order','cust_prod_aog','order_dow','week_day_name','cal_month_nbr','cal_month_name','cal_qtr_nbr','cal_year_nbr','cal_week_nbr','rank_cust','train_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['days_since_prior_order'] = data['days_since_prior_order'].replace('\\\\N', 99999 )\n",
    "data['days_since_prior_order'] =  data['days_since_prior_order'].apply(lambda x:float(x))\n",
    "data['days_since_prior_order']= data['days_since_prior_order'].replace(99999 , np.NaN )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['visit_date'] = data['visit_date'].replace('\\\\N', np.NaN )\n",
    "data['visit_date'] =  pd.to_datetime(data['visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LP_check = data[(data['user_id'] == 121800790)]\n",
    "LP_check.to_csv('LP_check5.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['add_to_cart_order'] = data.groupby(['user_id', 'order_id'])['product_id'].rank(method='dense').astype(int)\n",
    "data['eval_set'] = ['prior' if x > 1 else 'train' for x in data['train_test']]\n",
    "data.rename(columns={'order_id': 'order_number'}, inplace=True)\n",
    "data['order_id']= data.apply(lambda x:'%s_%s' % (x['user_id'],x['order_number']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'user_id', u'visit_date', u'product_id', u'units', u'dollars',\n",
       "       u'cust_lead_date', u'cust_prod_lead_date', u'order_number',\n",
       "       u'reordered', u'days_since_prior_order', u'cust_prod_aog', u'order_dow',\n",
       "       u'week_day_name', u'cal_month_nbr', u'cal_month_name', u'cal_qtr_nbr',\n",
       "       u'cal_year_nbr', u'cal_week_nbr', u'rank_cust', u'train_test',\n",
       "       u'add_to_cart_order', u'eval_set', u'order_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a0v0022/anaconda/lib/python2.7/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/a0v0022/anaconda/lib/python2.7/site-packages/numpy/core/_methods.py:135: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,16):\n",
    "    #Train Dataset\n",
    "    init_date_train = min(data['visit_date'])+timedelta(days=i)\n",
    "    last_date_train = init_date_train + timedelta(days=120)\n",
    "    init_date_test = init_date_train + timedelta(days=15)\n",
    "    last_date_test = last_date_train + timedelta(days=15)\n",
    "    train_Y_date = last_date_train + timedelta(days=1)\n",
    "    test_Y_date = last_date_test + timedelta(days=1)\n",
    "    order_prior = data[data['visit_date'] >= init_date_train]\n",
    "    order_prior = order_prior[order_prior['visit_date'] <= last_date_train]\n",
    "    max(order_prior['visit_date'])\n",
    "    order_train = data[data['visit_date'] == train_Y_date]\n",
    "    order_train = order_train.loc[:,['user_id','product_id','reordered']].drop_duplicates()\n",
    "    cust_map = data.loc[:,['user_id', 'rank_cust']]\n",
    "    cust_map = cust_map.drop_duplicates()\n",
    "    cust_map = pd.DataFrame(cust_map)\n",
    "    orders = data.loc[: , ['user_id' ,'visit_date', 'order_id' ,'order_number', 'days_since_prior_order','eval_set', 'order_dow']]\n",
    "    orders = orders.drop_duplicates()\n",
    "    orders = pd.DataFrame(orders)\n",
    "    cust_map.shape\n",
    "    order_comsum = orders[['user_id', 'order_number', 'days_since_prior_order']].groupby(['user_id', 'order_number'])\\\n",
    "    ['days_since_prior_order'].sum().groupby(level=[0]).cumsum().reset_index().rename(columns={'days_since_prior_order':'days_since_prior_order_comsum'})\n",
    "    order_comsum.columns = ['user_id', 'order_number', 'days_since_prior_order_comsum']\n",
    "    order_comsum = pd.merge(order_comsum, orders, on=['user_id', 'order_number'])[['user_id', 'order_number', 'days_since_prior_order_comsum', 'order_id']]\n",
    "    order_product = pd.merge(order_prior, orders, on='order_id')[['order_id', 'product_id']]\n",
    "    order_product = pd.merge(order_product, order_comsum, on='order_id')\n",
    "    temp = order_product.groupby(['user_id', 'product_id', 'order_number'])['days_since_prior_order_comsum'].sum().groupby(level=[0, 1]).apply(lambda x: np.diff(np.nan_to_num(x)))\n",
    "    temp = temp.to_frame('periods').reset_index()\n",
    "    aggregated = temp.copy()\n",
    "    aggregated['last'] = aggregated.periods.apply(lambda x: x[-1] if len(x) > 0 else np.nan)\n",
    "    aggregated['prev1'] = aggregated.periods.apply(lambda x: x[-2] if len(x) > 1 else np.nan)\n",
    "    aggregated['prev2'] = aggregated.periods.apply(lambda x: x[-3] if len(x) > 2 else np.nan)\n",
    "    aggregated['median'] = aggregated.periods.apply(lambda x: np.median(x[:-1]))\n",
    "    aggregated['mean'] = aggregated.periods.apply(lambda x: np.mean(x[:-1]))\n",
    "    aggregated['sdev'] = aggregated.periods.apply(lambda x : np.std(x[:-1]))\n",
    "    aggregated.drop('periods', axis=1, inplace=True)\n",
    "    LP_check = aggregated[(aggregated['user_id'] == 121800790)]\n",
    "    LP_check.to_csv('LP_check4.csv', index=False, header=True)\n",
    "    prob = order_prior\n",
    "    prob = prob.groupby(['product_id', 'user_id'],as_index=False).agg({'reordered':'sum', 'train_test': 'size'})\n",
    "    prob.rename(columns={'sum': 'reordered', 'train_test': 'total'}, inplace=True)\n",
    "    prob.reordered = (prob.reordered > 0).astype(np.float32)\n",
    "    prob.total = (prob.total > 0).astype(np.float32)\n",
    "    prob['reorder_prob'] = prob.reordered / prob.total\n",
    "    prob = prob.groupby('product_id').agg({'reorder_prob': 'mean'}).rename(columns={'mean': 'reorder_prob'}).reset_index()\n",
    "    prod_stat = order_prior.groupby('product_id').agg({'reordered': ['sum', 'size']})\n",
    "    prod_stat.columns = prod_stat.columns.levels[1]\n",
    "    prod_stat.rename(columns={'sum':'prod_reorders',\n",
    "                              'size':'prod_orders'}, inplace=True)\n",
    "    prod_stat.reset_index(inplace=True)\n",
    "    prod_stat['reorder_ration'] = prod_stat['prod_reorders'] / prod_stat['prod_orders']\n",
    "    prod_stat = pd.merge(prod_stat, prob, on='product_id')\n",
    "    user_stat = orders.loc[orders.eval_set == 'prior', :].groupby('user_id').agg({'order_number': 'max',\n",
    "                                                                                  'days_since_prior_order': ['sum',\n",
    "                                                                                                             'mean',\n",
    "                                                                                                             'median']})\n",
    "    user_stat.columns = user_stat.columns.droplevel(0)\n",
    "    user_stat.rename(columns={'max': 'user_orders',\n",
    "                              'sum': 'user_order_starts_at',\n",
    "                              'mean': 'user_mean_days_since_prior',\n",
    "                              'median': 'user_median_days_since_prior'}, inplace=True)\n",
    "    user_stat.reset_index(inplace=True)\n",
    "    orders_products =  order_prior\n",
    "    user_order_stat = orders_products.loc[order_prior.eval_set == 'prior', :].groupby('user_id').agg({'user_id': 'size',\n",
    "                                                              'reordered': 'sum',\n",
    "                                                              \"product_id\": lambda x: x.nunique()})\n",
    "    user_order_stat.rename(columns={'user_id': 'user_total_products',\n",
    "                                    'product_id': 'user_distinct_products',\n",
    "                                    'reordered': 'user_reorder_ratio'}, inplace=True)\n",
    "    user_order_stat.reset_index(inplace=True)\n",
    "    user_order_stat.user_reorder_ratio = user_order_stat.user_reorder_ratio / user_order_stat.user_total_products\n",
    "    user_stat = pd.merge(user_stat, user_order_stat, on='user_id')\n",
    "    user_stat['user_average_basket'] = user_stat.user_total_products / user_stat.user_orders\n",
    "    prod_usr = order_prior.groupby(['product_id']).agg({'user_id': lambda x: x.nunique()})\n",
    "    prod_usr.rename(columns={'user_id':'prod_users_unq'}, inplace=True)\n",
    "    prod_usr.reset_index(inplace=True)\n",
    "    prod_usr_reordered = order_prior.loc[order_prior.reordered, :].groupby(['product_id']).agg({'user_id': lambda x: x.nunique()})\n",
    "    prod_usr_reordered.rename(columns={'user_id': 'prod_users_unq_reordered'}, inplace=True)\n",
    "    prod_usr_reordered.reset_index(inplace=True)\n",
    "    orders_products = order_prior\n",
    "    order_stat = orders_products.groupby('order_id').agg({'order_id': 'size'}) \\\n",
    "        .rename(columns={'order_id': 'order_size'}).reset_index()\n",
    "\n",
    "    orders_products = pd.merge(orders_products, order_stat, on='order_id')\n",
    "    data1 = orders_products.groupby(['user_id', 'product_id']).agg({'user_id': 'size',\n",
    "                                                                   'order_number': ['min', 'max'],\n",
    "                                                                   'days_since_prior_order': ['mean', 'median'],\n",
    "                                                                   'order_dow': ['mean', 'median'],\n",
    "                                                                   'reordered': ['sum']})\n",
    "    data1.columns = data1.columns.droplevel(0)\n",
    "    data1.columns = [ 'up_first_order', 'up_last_order', 'order_dow_mean', 'order_dow_median','up_orders',\n",
    "                    'days_since_prior_order_mean', 'days_since_prior_order_median', 'reordered_sum']\n",
    "    data1['user_product_reordered_ratio'] = (data1.reordered_sum + 1.0) / data1.up_orders\n",
    "    data1.reset_index(inplace=True)\n",
    "\n",
    "    data1 = pd.merge(data1, prod_stat, on='product_id')\n",
    "    data1 = pd.merge(data1, user_stat, on='user_id')\n",
    "\n",
    "    data1['up_order_rate'] = data1.up_orders / data1.user_orders\n",
    "    data1['up_orders_since_last_order'] = data1.user_orders - data1.up_last_order\n",
    "    data1['up_order_rate_since_first_order'] = data1.user_orders / (data1.user_orders - data1.up_first_order + 1)\n",
    "    orders1 = orders[orders['visit_date'] >= init_date_train]\n",
    "    orders1 = orders1[orders1['visit_date'] <= last_date_train]\n",
    "    orders_user = orders1[['order_id', 'user_id']]\n",
    "    labels = pd.merge(order_prior, orders_user, on=['user_id','order_id'])\n",
    "    labels = labels.loc[:, ['user_id', 'product_id']].drop_duplicates()\n",
    "    train = pd.merge(labels, prod_usr, on='product_id')\n",
    "    train = pd.merge(train, prod_usr_reordered, on='product_id', how='left')\n",
    "    train.prod_users_unq_reordered.fillna(0, inplace=True)\n",
    "    train = pd.merge(train, data1, on=['product_id', 'user_id'])\n",
    "    train = pd.merge(train, aggregated, on=['user_id',  'product_id'])\n",
    "    LP_check = train[(train['user_id'] == 121800790)]\n",
    "    LP_check.to_csv('LP_check6.csv', index=False, header=True)\n",
    "    dep = order_train.loc[:,['user_id','product_id','reordered']]\n",
    "    train = pd.merge(train, dep,on=['user_id',  'product_id'], how='left')\n",
    "    train.reordered.fillna(0, inplace=True)\n",
    "    train.fillna(999, inplace=True)\n",
    "    train = pd.merge(train, cust_map, on='user_id')\n",
    "    train.shape\n",
    "    LP_check = train[(train['user_id'] == 121800790)]\n",
    "    LP_check.to_csv('LP_check7.csv', index=False, header=True)\n",
    "    train[\"counter\"]=i\n",
    "    if i==0:\n",
    "        Base_train=train\n",
    "    else :\n",
    "        Base_train = Base_train.append(train,ignore_index=True)\n",
    "       \n",
    "    #Test Data Creation\n",
    "    \n",
    "    order_prior = data[data['visit_date'] >= init_date_test]\n",
    "    order_prior = order_prior[order_prior['visit_date'] <= last_date_test]\n",
    "    min(order_prior['visit_date'])\n",
    "    order_train = data[data['visit_date'] == test_Y_date]\n",
    "    order_train = order_train.loc[:,['user_id','product_id','reordered']].drop_duplicates()\n",
    "    data.columns\n",
    "    cust_map = data.loc[:,['user_id', 'rank_cust']]\n",
    "    cust_map = cust_map.drop_duplicates()\n",
    "    cust_map = pd.DataFrame(cust_map)\n",
    "    orders = data.loc[: , ['user_id' ,'visit_date', 'order_id' ,'order_number', 'days_since_prior_order','eval_set', 'order_dow']]\n",
    "    orders = orders.drop_duplicates()\n",
    "    orders = pd.DataFrame(orders)\n",
    "    cust_map.shape\n",
    "    order_comsum = orders[['user_id', 'order_number', 'days_since_prior_order']].groupby(['user_id', 'order_number'])\\\n",
    "    ['days_since_prior_order'].sum().groupby(level=[0]).cumsum().reset_index().rename(columns={'days_since_prior_order':'days_since_prior_order_comsum'})\n",
    "    order_comsum.columns = ['user_id', 'order_number', 'days_since_prior_order_comsum']\n",
    "    order_comsum = pd.merge(order_comsum, orders, on=['user_id', 'order_number'])[['user_id', 'order_number', 'days_since_prior_order_comsum', 'order_id']]\n",
    "    order_product = pd.merge(order_prior, orders, on='order_id')[['order_id', 'product_id']]\n",
    "    order_product = pd.merge(order_product, order_comsum, on='order_id')\n",
    "    temp = order_product.groupby(['user_id', 'product_id', 'order_number'])['days_since_prior_order_comsum'].sum().groupby(level=[0, 1]).apply(lambda x: np.diff(np.nan_to_num(x)))\n",
    "    temp = temp.to_frame('periods').reset_index()\n",
    "    aggregated = temp.copy()\n",
    "    aggregated['last'] = aggregated.periods.apply(lambda x: x[-1] if len(x) > 0 else np.nan)\n",
    "    aggregated['prev1'] = aggregated.periods.apply(lambda x: x[-2] if len(x) > 1 else np.nan)\n",
    "    aggregated['prev2'] = aggregated.periods.apply(lambda x: x[-3] if len(x) > 2 else np.nan)\n",
    "    aggregated['median'] = aggregated.periods.apply(lambda x: np.median(x[:-1]))\n",
    "    aggregated['mean'] = aggregated.periods.apply(lambda x: np.mean(x[:-1]))\n",
    "    aggregated['sdev'] = aggregated.periods.apply(lambda x : np.std(x[:-1]))\n",
    "    aggregated.drop('periods', axis=1, inplace=True)\n",
    "    prob = order_prior\n",
    "    prob = prob.groupby(['product_id', 'user_id'],as_index=False).agg({'reordered':'sum', 'train_test': 'size'})\n",
    "    prob.rename(columns={'sum': 'reordered', 'train_test': 'total'}, inplace=True)\n",
    "    prob.reordered = (prob.reordered > 0).astype(np.float32)\n",
    "    prob.total = (prob.total > 0).astype(np.float32)\n",
    "    prob['reorder_prob'] = prob.reordered / prob.total\n",
    "    prob = prob.groupby('product_id').agg({'reorder_prob': 'mean'}).rename(columns={'mean': 'reorder_prob'}).reset_index()\n",
    "    prod_stat = order_prior.groupby('product_id').agg({'reordered': ['sum', 'size']})\n",
    "    prod_stat.columns = prod_stat.columns.levels[1]\n",
    "    prod_stat.rename(columns={'sum':'prod_reorders',\n",
    "                              'size':'prod_orders'}, inplace=True)\n",
    "    prod_stat.reset_index(inplace=True)\n",
    "    prod_stat['reorder_ration'] = prod_stat['prod_reorders'] / prod_stat['prod_orders']\n",
    "    prod_stat = pd.merge(prod_stat, prob, on='product_id')\n",
    "    user_stat = orders.loc[orders.eval_set == 'prior', :].groupby('user_id').agg({'order_number': 'max',\n",
    "                                                                                  'days_since_prior_order': ['sum',\n",
    "                                                                                                             'mean',\n",
    "                                                                                                             'median']})\n",
    "    user_stat.columns = user_stat.columns.droplevel(0)\n",
    "    user_stat.rename(columns={'max': 'user_orders',\n",
    "                              'sum': 'user_order_starts_at',\n",
    "                              'mean': 'user_mean_days_since_prior',\n",
    "                              'median': 'user_median_days_since_prior'}, inplace=True)\n",
    "    user_stat.reset_index(inplace=True)\n",
    "    orders_products =  order_prior\n",
    "    user_order_stat = orders_products.loc[order_prior.eval_set == 'prior', :].groupby('user_id').agg({'user_id': 'size',\n",
    "                                                              'reordered': 'sum',\n",
    "                                                              \"product_id\": lambda x: x.nunique()})\n",
    "    user_order_stat.rename(columns={'user_id': 'user_total_products',\n",
    "                                    'product_id': 'user_distinct_products',\n",
    "                                    'reordered': 'user_reorder_ratio'}, inplace=True)\n",
    "    user_order_stat.reset_index(inplace=True)\n",
    "    user_order_stat.user_reorder_ratio = user_order_stat.user_reorder_ratio / user_order_stat.user_total_products\n",
    "    user_stat = pd.merge(user_stat, user_order_stat, on='user_id')\n",
    "    user_stat['user_average_basket'] = user_stat.user_total_products / user_stat.user_orders\n",
    "    prod_usr = order_prior.groupby(['product_id']).agg({'user_id': lambda x: x.nunique()})\n",
    "    prod_usr.rename(columns={'user_id':'prod_users_unq'}, inplace=True)\n",
    "    prod_usr.reset_index(inplace=True)\n",
    "    prod_usr_reordered = order_prior.loc[order_prior.reordered, :].groupby(['product_id']).agg({'user_id': lambda x: x.nunique()})\n",
    "    prod_usr_reordered.rename(columns={'user_id': 'prod_users_unq_reordered'}, inplace=True)\n",
    "    prod_usr_reordered.reset_index(inplace=True)\n",
    "    orders_products = order_prior\n",
    "    order_stat = orders_products.groupby('order_id').agg({'order_id': 'size'}) \\\n",
    "        .rename(columns={'order_id': 'order_size'}).reset_index()\n",
    "\n",
    "    orders_products = pd.merge(orders_products, order_stat, on='order_id')\n",
    "    data1 = orders_products.groupby(['user_id', 'product_id']).agg({'user_id': 'size',\n",
    "                                                                   'order_number': ['min', 'max'],\n",
    "                                                                   'days_since_prior_order': ['mean', 'median'],\n",
    "                                                                   'order_dow': ['mean', 'median'],\n",
    "                                                                   'reordered': ['sum']})\n",
    "    data1.columns = data1.columns.droplevel(0)\n",
    "    data1.columns = [ 'up_first_order', 'up_last_order', 'order_dow_mean', 'order_dow_median','up_orders',\n",
    "                    'days_since_prior_order_mean', 'days_since_prior_order_median', 'reordered_sum']\n",
    "    data1['user_product_reordered_ratio'] = (data1.reordered_sum + 1.0) / data1.up_orders\n",
    "    data1.reset_index(inplace=True)\n",
    "\n",
    "    data1 = pd.merge(data1, prod_stat, on='product_id')\n",
    "    data1 = pd.merge(data1, user_stat, on='user_id')\n",
    "\n",
    "    data1['up_order_rate'] = data1.up_orders / data1.user_orders\n",
    "    data1['up_orders_since_last_order'] = data1.user_orders - data1.up_last_order\n",
    "    data1['up_order_rate_since_first_order'] = data1.user_orders / (data1.user_orders - data1.up_first_order + 1)\n",
    "    orders1 = orders[orders['visit_date'] >= init_date_test]\n",
    "    orders1 = orders1[orders1['visit_date'] <= last_date_test]\n",
    "    orders_user = orders1[['order_id', 'user_id']]\n",
    "    labels = pd.merge(order_prior, orders_user, on=['user_id','order_id'])\n",
    "    labels = labels.loc[:, ['user_id', 'product_id']].drop_duplicates()\n",
    "    test = pd.merge(labels, prod_usr, on='product_id')\n",
    "    test = pd.merge(test, prod_usr_reordered, on='product_id', how='left')\n",
    "    test.prod_users_unq_reordered.fillna(0, inplace=True)\n",
    "    test = pd.merge(test, data1, on=['product_id', 'user_id'])\n",
    "    test = pd.merge(test, aggregated, on=['user_id',  'product_id'])\n",
    "    dep = order_train.loc[:,['user_id','product_id','reordered']]\n",
    "    test = pd.merge(test, dep,on=['user_id',  'product_id'], how='left')\n",
    "    test.reordered.fillna(0, inplace=True)\n",
    "    test.fillna(999, inplace=True)\n",
    "    test = pd.merge(test, cust_map, on='user_id')\n",
    "    test.shape\n",
    "    LP_check = test[(test['user_id'] == 121800790)]\n",
    "    LP_check.to_csv('LP_check8.csv', index=False, header=True)\n",
    "    test[\"counter\"]=i\n",
    "    if i==0:\n",
    "        Base_test=test\n",
    "    else :\n",
    "        Base_test = Base_test.append(test,ignore_index =True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25841251, 37)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Base_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25894409, 37)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Base_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Base_train.to_csv('Base_train.csv', index=False, header=True)\n",
    "Base_test.to_csv('Base_test.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LP_check = Base_train[(Base_train['user_id'] == 121800790)]\n",
    "LP_check.to_csv('LP_check0.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train1 = Base_train[(Base_train['counter'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1615609, 37)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = train.iloc[:, 2:34]\n",
    "test_X = test.iloc[:, 2:34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = list(train_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prod_users_unq',\n",
       " 'prod_users_unq_reordered',\n",
       " 'up_first_order',\n",
       " 'up_last_order',\n",
       " 'order_dow_mean',\n",
       " 'order_dow_median',\n",
       " 'up_orders',\n",
       " 'days_since_prior_order_mean',\n",
       " 'days_since_prior_order_median',\n",
       " 'reordered_sum',\n",
       " 'user_product_reordered_ratio',\n",
       " 'prod_reorders',\n",
       " 'prod_orders',\n",
       " 'reorder_ration',\n",
       " 'reorder_prob',\n",
       " 'user_order_starts_at',\n",
       " 'user_mean_days_since_prior',\n",
       " 'user_median_days_since_prior',\n",
       " 'user_orders',\n",
       " 'user_total_products',\n",
       " 'user_distinct_products',\n",
       " 'user_reorder_ratio',\n",
       " 'user_average_basket',\n",
       " 'up_order_rate',\n",
       " 'up_orders_since_last_order',\n",
       " 'up_order_rate_since_first_order',\n",
       " 'last',\n",
       " 'prev1',\n",
       " 'prev2',\n",
       " 'median',\n",
       " 'mean',\n",
       " 'sdev']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = train_X.as_matrix()\n",
    "test_X = test_X.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True, ...,  True,  True,  True], dtype=bool)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_Y = train['reordered']\n",
    "test_Y = test['reordered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = np.random.choice([False, True], len(train_X), p=[0.75, 0.25])\n",
    "not_mask = ~mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xg_train = xgb.DMatrix(train_X[not_mask], label=train_Y[not_mask], feature_names=feature_names)\n",
    "xg_val = xgb.DMatrix(train_X[mask], label=train_Y[mask], feature_names=feature_names)\n",
    "xg_test = xgb.DMatrix(test_X , feature_names=feature_names)\n",
    "\n",
    "param = {}\n",
    "param['objective'] = 'binary:logistic'\n",
    "param['eta'] = 0.05\n",
    "param['max_depth'] = 6\n",
    "param['silent'] = 1\n",
    "param['eval_metric'] = \"logloss\"\n",
    "param['subsample'] = 0.75\n",
    "param['colsample_bytree'] = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.646596+1.26079e-05\ttest-logloss:0.646602+2.17348e-05\n",
      "[1]\ttrain-logloss:0.604479+1.31757e-05\ttest-logloss:0.604489+5.2458e-05\n",
      "[2]\ttrain-logloss:0.566198+2.54527e-05\ttest-logloss:0.566213+7.13033e-05\n",
      "[3]\ttrain-logloss:0.531267+4.01368e-05\ttest-logloss:0.531288+8.90092e-05\n",
      "[4]\ttrain-logloss:0.499294+2.28263e-05\ttest-logloss:0.499321+0.000126563\n"
     ]
    }
   ],
   "source": [
    "cv = xgb.cv(param, xg_train, 5, nfold=5, early_stopping_rounds=5, verbose_eval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-logloss:0.646592\n",
      "[1]\teval-logloss:0.604488\n",
      "[2]\teval-logloss:0.5662\n",
      "[3]\teval-logloss:0.531269\n",
      "[4]\teval-logloss:0.49927\n"
     ]
    }
   ],
   "source": [
    "evallist  = [(xg_val, 'eval')]\n",
    "bst = xgb.train( param, xg_train, 5, evallist )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>order_dow_mean</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>order_dow_median</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>user_mean_days_since_prior</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>up_orders</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_reorder_ratio</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_average_basket</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>median</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>user_distinct_products</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>up_order_rate</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>last</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>days_since_prior_order_median</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>user_orders</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>up_first_order</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_median_days_since_prior</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>reorder_prob</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>days_since_prior_order_mean</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prod_reorders</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>user_total_products</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>up_order_rate_since_first_order</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>mean</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>up_last_order</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>reorder_ration</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>prod_users_unq</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>user_order_starts_at</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sdev</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>up_orders_since_last_order</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>prev2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            feature  importance\n",
       "19                   order_dow_mean          28\n",
       "12                 order_dow_median          27\n",
       "22       user_mean_days_since_prior          17\n",
       "0                         up_orders          16\n",
       "4                user_reorder_ratio          16\n",
       "1               user_average_basket          14\n",
       "23                           median          12\n",
       "10           user_distinct_products          11\n",
       "24                    up_order_rate          10\n",
       "21                             last           8\n",
       "15    days_since_prior_order_median           7\n",
       "7                       user_orders           7\n",
       "9                    up_first_order           6\n",
       "3      user_median_days_since_prior           6\n",
       "8                      reorder_prob           5\n",
       "18      days_since_prior_order_mean           5\n",
       "6                     prod_reorders           4\n",
       "16              user_total_products           4\n",
       "20  up_order_rate_since_first_order           3\n",
       "26                             mean           3\n",
       "17                    up_last_order           3\n",
       "11                   reorder_ration           3\n",
       "5                    prod_users_unq           3\n",
       "14             user_order_starts_at           2\n",
       "2                              sdev           2\n",
       "25       up_orders_since_last_order           1\n",
       "13                            prev2           1"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imp = pd.DataFrame(bst.get_fscore().items(), columns=['feature','importance'])\n",
    "feature_imp.sort('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = bst.predict( xg_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame(preds)\n",
    "prediction.columns = ['reordered_p']\n",
    "Submit = test.iloc[:,(0,1,34)]\n",
    "submit1 = pd.concat([Submit, prediction], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1optimization = submit1\n",
    "f1optimization['rank'] = f1optimization.groupby(['user_id'])['reordered_p'].rank(method = 'dense' , ascending = False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def applyParallel(dfGrouped, func):\n",
    "    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)\n",
    "    return pd.concat(retLst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class F1Optimizer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def get_expectations(P, pNone=None):\n",
    "        expectations = []\n",
    "        P = np.sort(P)[::-1]\n",
    "\n",
    "        n = np.array(P).shape[0]\n",
    "        DP_C = np.zeros((n + 2, n + 1))\n",
    "        if pNone is None:\n",
    "            pNone = (1.0 - P).prod()\n",
    "\n",
    "        DP_C[0][0] = 1.0\n",
    "        for j in range(1, n):\n",
    "            DP_C[0][j] = (1.0 - P[j - 1]) * DP_C[0, j - 1]\n",
    "\n",
    "        for i in range(1, n + 1):\n",
    "            DP_C[i, i] = DP_C[i - 1, i - 1] * P[i - 1]\n",
    "            for j in range(i + 1, n + 1):\n",
    "                DP_C[i, j] = P[j - 1] * DP_C[i - 1, j - 1] + (1.0 - P[j - 1]) * DP_C[i, j - 1]\n",
    "\n",
    "        DP_S = np.zeros((2 * n + 1,))\n",
    "        DP_SNone = np.zeros((2 * n + 1,))\n",
    "        for i in range(1, 2 * n + 1):\n",
    "            DP_S[i] = 1. / (1. * i)\n",
    "            DP_SNone[i] = 1. / (1. * i + 1)\n",
    "        for k in range(n + 1)[::-1]:\n",
    "            f1 = 0\n",
    "            f1None = 0\n",
    "            for k1 in range(n + 1):\n",
    "                f1 += 2 * k1 * DP_C[k1][k] * DP_S[k + k1]\n",
    "                f1None += 2 * k1 * DP_C[k1][k] * DP_SNone[k + k1]\n",
    "            for i in range(1, 2 * k - 1):\n",
    "                DP_S[i] = (1 - P[k - 1]) * DP_S[i] + P[k - 1] * DP_S[i + 1]\n",
    "                DP_SNone[i] = (1 - P[k - 1]) * DP_SNone[i] + P[k - 1] * DP_SNone[i + 1]\n",
    "            expectations.append([f1None + 2 * pNone / (2 + k), f1])\n",
    "\n",
    "        return np.array(expectations[::-1]).T\n",
    "\n",
    "    @staticmethod\n",
    "    def maximize_expectation(P, pNone=None):\n",
    "        expectations = F1Optimizer.get_expectations(P, pNone)\n",
    "\n",
    "        ix_max = np.unravel_index(expectations.argmax(), expectations.shape)\n",
    "        max_f1 = expectations[ix_max]\n",
    "\n",
    "        predNone = True if ix_max[0] == 0 else False\n",
    "        best_k = ix_max[1]\n",
    "\n",
    "        return best_k, predNone, max_f1\n",
    "\n",
    "    @staticmethod\n",
    "    def _F1(tp, fp, fn):\n",
    "        return 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "    @staticmethod\n",
    "    def _Fbeta(tp, fp, fn, beta=1.0):\n",
    "        beta_squared = beta ** 2\n",
    "        return (1.0 + beta_squared) * tp / ((1.0 + beta_squared) * tp + fp + beta_squared * fn)\n",
    "\n",
    "\n",
    "def print_best_prediction(P, pNone=None):\n",
    "    #print(\"Maximize F1-Expectation\")\n",
    "    #print(\"=\" * 23)\n",
    "    P = np.sort(P)[::-1]\n",
    "    n = P.shape[0]\n",
    "    L = ['L{}'.format(i + 1) for i in range(n)]\n",
    "\n",
    "    if pNone is None:\n",
    "        #print(\"Estimate p(None|x) as (1-p_1)*(1-p_2)*...*(1-p_n)\")\n",
    "        pNone = (1.0 - P).prod()\n",
    "\n",
    "    PL = ['p({}|x)={}'.format(l, p) for l, p in zip(L, P)]\n",
    "    #print(\"Posteriors: {} (n={})\".format(PL, n))\n",
    "    #print(\"p(None|x)={}\".format(pNone))\n",
    "\n",
    "    opt = F1Optimizer.maximize_expectation(P, pNone)\n",
    "    best_prediction = ['None'] if opt[1] else []\n",
    "    best_prediction += (L[:opt[0]])\n",
    "    f1_max = opt[2]\n",
    "    k_max = opt[0]\n",
    "    pred_none = opt[1]\n",
    "    #print(\"Prediction {} yields best E[F1] of {} with max_k of {} and pred_none is {} \\n\".format(best_prediction, f1_max , k_max,pred_none ))\n",
    "    return k_max, pred_none\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_products(df):\n",
    "    prob = df.reordered_p.values\n",
    "    sort_index = np.argsort(prob)[::-1]\n",
    "    values = print_best_prediction(df.reordered_p.values)\n",
    "    df.loc[:, 'k_max'] = values[0]\n",
    "    df.loc[:, 'pred_none'] = values[1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1 = applyParallel(f1optimization.groupby(f1optimization.user_id) , create_products).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LP_check = data1[(data1['user_id'] == 121800790)]\n",
    "LP_check.to_csv('LP_check9.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit1['Reorder_predict'] = [1 if x > 0.5 else 0 for x in submit1['Probab']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = submit1['Reorder_predict'] \n",
    "y_test = test['reordered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [ 0.98546605  0.01972387]\n",
      "recall: [  9.99373664e-01   8.54299261e-04]\n",
      "fscore: [ 0.99237113  0.00163767]\n",
      "support: [1587007   23411]\n"
     ]
    }
   ],
   "source": [
    "precision, recall, fscore, support = score(y_test, predicted)\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
