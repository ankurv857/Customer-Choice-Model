{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/a0v0022/Documents/Logical_Purchase'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.sparse import dok_matrix, coo_matrix\n",
    "from sklearn.utils.multiclass import  type_of_target\n",
    "#from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import itertools\n",
    "from sklearn.ensemble import GradientBoostingClassifier as gbm \n",
    "from sklearn.cross_validation import KFold\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Logical_purchase_1000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['add_to_cart_order'] = data.groupby(['user_id', 'order_id'])['product_id'].rank(method='dense').astype(int)\n",
    "data['eval_set'] = ['prior' if x > 1 else 'train' for x in data['train_test']]\n",
    "data.rename(columns={'order_id': 'order_number'}, inplace=True)\n",
    "data['order_id']= data.apply(lambda x:'%s_%s' % (x['user_id'],x['order_number']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order_prior = data[data['eval_set'] == 'prior']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order_train = data[data['train_test'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'user_id', u'visit_date', u'product_id', u'units', u'dollars',\n",
       "       u'cust_lead_date', u'cust_prod_lead_date', u'order_number',\n",
       "       u'reordered', u'days_since_prior_order', u'cust_prod_aog', u'order_dow',\n",
       "       u'week_day_name', u'cal_month_nbr', u'cal_month_name', u'cal_qtr_nbr',\n",
       "       u'cal_year_nbr', u'cal_week_nbr', u'rank_cust', u'train_test',\n",
       "       u'add_to_cart_order', u'eval_set', u'order_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cust_map = data.loc[:,['user_id', 'rank_cust']]\n",
    "cust_map = cust_map.drop_duplicates()\n",
    "cust_map = pd.DataFrame(cust_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orders = data.loc[: , ['user_id' , 'order_id' ,'order_number', 'days_since_prior_order','eval_set', 'order_dow']]\n",
    "orders = orders.drop_duplicates()\n",
    "orders = pd.DataFrame(orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order_comsum = orders[['user_id', 'order_number', 'days_since_prior_order']].groupby(['user_id', 'order_number'])\\\n",
    "['days_since_prior_order'].sum().groupby(level=[0]).cumsum().reset_index().rename(columns={'days_since_prior_order':'days_since_prior_order_comsum'})\n",
    "order_comsum.columns = ['user_id', 'order_number', 'days_since_prior_order_comsum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order_comsum = pd.merge(order_comsum, orders, on=['user_id', 'order_number'])[['user_id', 'order_number', 'days_since_prior_order_comsum', 'order_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_product = pd.merge(order_prior, orders, on='order_id')[['order_id', 'product_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order_product = pd.merge(order_product, order_comsum, on='order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = order_product.groupby(['user_id', 'product_id', 'order_number'])['days_since_prior_order_comsum'].sum().groupby(level=[0, 1]).apply(lambda x: np.diff(np.nan_to_num(x)))\n",
    "temp = temp.to_frame('periods').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggregated = temp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a0v0022/anaconda/lib/python2.7/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "aggregated['last'] = aggregated.periods.apply(lambda x: x[-1] if len(x) > 0 else np.nan)\n",
    "aggregated['prev1'] = aggregated.periods.apply(lambda x: x[-2] if len(x) > 1 else np.nan)\n",
    "aggregated['prev2'] = aggregated.periods.apply(lambda x: x[-3] if len(x) > 2 else np.nan)\n",
    "aggregated['median'] = aggregated.periods.apply(lambda x: np.median(x[:-1]))\n",
    "aggregated['mean'] = aggregated.periods.apply(lambda x: np.mean(x[:-1]))\n",
    "aggregated.drop('periods', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LP_check = aggregated[(aggregated['user_id'] == 1005126395)]\n",
    "LP_check.to_csv('LP_check4.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob = order_prior\n",
    "prob = prob.groupby(['product_id', 'user_id'],as_index=False).agg({'reordered':'sum', 'train_test': 'size'})\n",
    "prob.rename(columns={'sum': 'reordered', 'train_test': 'total'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob.reordered = (prob.reordered > 0).astype(np.float32)\n",
    "prob.total = (prob.total > 0).astype(np.float32)\n",
    "prob['reorder_prob'] = prob.reordered / prob.total\n",
    "prob = prob.groupby('product_id').agg({'reorder_prob': 'mean'}).rename(columns={'mean': 'reorder_prob'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prod_stat = order_prior.groupby('product_id').agg({'reordered': ['sum', 'size']})\n",
    "prod_stat.columns = prod_stat.columns.levels[1]\n",
    "prod_stat.rename(columns={'sum':'prod_reorders',\n",
    "                          'size':'prod_orders'}, inplace=True)\n",
    "prod_stat.reset_index(inplace=True)\n",
    "prod_stat['reorder_ration'] = prod_stat['prod_reorders'] / prod_stat['prod_orders']\n",
    "prod_stat = pd.merge(prod_stat, prob, on='product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_stat = orders.loc[orders.eval_set == 'prior', :].groupby('user_id').agg({'order_number': 'max',\n",
    "                                                                              'days_since_prior_order': ['sum',\n",
    "                                                                                                         'mean',\n",
    "                                                                                                         'median']})\n",
    "user_stat.columns = user_stat.columns.droplevel(0)\n",
    "user_stat.rename(columns={'max': 'user_orders',\n",
    "                          'sum': 'user_order_starts_at',\n",
    "                          'mean': 'user_mean_days_since_prior',\n",
    "                          'median': 'user_median_days_since_prior'}, inplace=True)\n",
    "user_stat.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orders_products =  order_prior\n",
    "user_order_stat = orders_products.loc[order_prior.eval_set == 'prior', :].groupby('user_id').agg({'user_id': 'size',\n",
    "                                                          'reordered': 'sum',\n",
    "                                                          \"product_id\": lambda x: x.nunique()})\n",
    "user_order_stat.rename(columns={'user_id': 'user_total_products',\n",
    "                                'product_id': 'user_distinct_products',\n",
    "                                'reordered': 'user_reorder_ratio'}, inplace=True)\n",
    "user_order_stat.reset_index(inplace=True)\n",
    "user_order_stat.user_reorder_ratio = user_order_stat.user_reorder_ratio / user_order_stat.user_total_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_stat = pd.merge(user_stat, user_order_stat, on='user_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_stat['user_average_basket'] = user_stat.user_total_products / user_stat.user_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prod_usr = order_prior.groupby(['product_id']).agg({'user_id': lambda x: x.nunique()})\n",
    "prod_usr.rename(columns={'user_id':'prod_users_unq'}, inplace=True)\n",
    "prod_usr.reset_index(inplace=True)\n",
    "prod_usr_reordered = order_prior.loc[order_prior.reordered, :].groupby(['product_id']).agg({'user_id': lambda x: x.nunique()})\n",
    "prod_usr_reordered.rename(columns={'user_id': 'prod_users_unq_reordered'}, inplace=True)\n",
    "prod_usr_reordered.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orders_products = order_prior\n",
    "order_stat = orders_products.groupby('order_id').agg({'order_id': 'size'}) \\\n",
    "    .rename(columns={'order_id': 'order_size'}).reset_index()\n",
    "\n",
    "orders_products = pd.merge(orders_products, order_stat, on='order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1 = orders_products.groupby(['user_id', 'product_id']).agg({'user_id': 'size',\n",
    "                                                               'order_number': ['min', 'max'],\n",
    "                                                               'days_since_prior_order': ['mean', 'median'],\n",
    "                                                               'order_dow': ['mean', 'median'],\n",
    "                                                               'reordered': ['sum']})\n",
    "data1.columns = data1.columns.droplevel(0)\n",
    "data1.columns = [ 'up_first_order', 'up_last_order', 'order_dow_mean', 'order_dow_median','up_orders',\n",
    "                'days_since_prior_order_mean', 'days_since_prior_order_median', 'reordered_sum']\n",
    "data1['user_product_reordered_ratio'] = (data1.reordered_sum + 1.0) / data1.up_orders\n",
    "data1.reset_index(inplace=True)\n",
    "\n",
    "data1 = pd.merge(data1, prod_stat, on='product_id')\n",
    "data1 = pd.merge(data1, user_stat, on='user_id')\n",
    "\n",
    "data1['up_order_rate'] = data1.up_orders / data1.user_orders\n",
    "data1['up_orders_since_last_order'] = data1.user_orders - data1.up_last_order\n",
    "data1['up_order_rate_since_first_order'] = data1.user_orders / (data1.user_orders - data1.up_first_order + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LP_check = data1[(data1['user_id'] == 1005126395)]\n",
    "LP_check.to_csv('LP_check5.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orders1 = orders.loc[orders.eval_set == 'prior', :]\n",
    "orders_user = orders1[['order_id', 'user_id']]\n",
    "labels = pd.merge(order_prior, orders_user, on=['user_id','order_id'])\n",
    "labels = labels.loc[:, ['user_id', 'product_id']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.merge(labels, prod_usr, on='product_id')\n",
    "train = pd.merge(train, prod_usr_reordered, on='product_id', how='left')\n",
    "train.prod_users_unq_reordered.fillna(0, inplace=True)\n",
    "train = pd.merge(train, data1, on=['product_id', 'user_id'])\n",
    "train = pd.merge(train, aggregated, on=['user_id',  'product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "LP_check = train[(train['user_id'] == 1005126395)]\n",
    "LP_check.to_csv('LP_check6.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dep = order_train.loc[:,['user_id','product_id','reordered']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train, dep,on=['user_id',  'product_id'], how='left')\n",
    "train.reordered.fillna(0, inplace=True)\n",
    "train.fillna(999, inplace=True)\n",
    "train = pd.merge(train, cust_map, on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = train[(train['rank_cust'] >= 900 )]\n",
    "train = train[(train['rank_cust'] < 900 )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83226, 35)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9331, 35)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LP_check = train[(train['user_id'] == 100012900)]\n",
    "LP_check.to_csv('LP_check7.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('train.csv', index=False, header=True)\n",
    "test.to_csv('test.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train.iloc[:,3:34]\n",
    "test_x = test.iloc[:,3:33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.5102           0.0021           24.97s\n",
      "         2           0.5077           0.0019           19.38s\n",
      "         3           0.5075           0.0019           16.72s\n",
      "         4           0.5063           0.0017           15.79s\n",
      "         5           0.5002           0.0017           14.82s\n",
      "         6           0.4999           0.0016           14.19s\n",
      "         7           0.5038           0.0015           13.63s\n",
      "         8           0.4992           0.0014           13.24s\n",
      "         9           0.4995           0.0014           12.83s\n",
      "        10           0.4956           0.0014           12.51s\n",
      "        20           0.4840           0.0010           10.82s\n",
      "        30           0.4726           0.0008            9.30s\n",
      "        40           0.4674           0.0006            7.89s\n",
      "        50           0.4594           0.0005            6.50s\n",
      "        60           0.4580           0.0004            5.25s\n",
      "        70           0.4542           0.0003            3.94s\n",
      "        80           0.4493           0.0003            2.64s\n",
      "        90           0.4446           0.0002            1.33s\n",
      "       100           0.4448           0.0002            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(init=None, learning_rate=0.01, loss='deviance',\n",
       "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              random_state=None, subsample=0.75, verbose=1,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = gbm( loss = 'deviance' ,\n",
    "          learning_rate= 0.01,n_estimators= 100, subsample= 0.75,verbose= 1)\n",
    "clf.fit(train_x.drop('reordered', axis = 1) ,train_x['reordered'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = clf.predict(test_x) \n",
    "out = pd.DataFrame(out) \n",
    "#test_x = pd.concat((test,out), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ankur_search(prob, dtype=np.float32):\n",
    "    size = len(prob)\n",
    "    fk = np.zeros((size + 1), dtype=dtype)\n",
    "    C = np.zeros((size + 1, size + 1), dtype=dtype)\n",
    "    S = np.empty((2 * size + 1), dtype=dtype)\n",
    "    S[:] = np.nan\n",
    "    for k in range(1, 2 * size + 1):\n",
    "        S[k] = 1./k\n",
    "    roots = (prob - 1.0) / prob\n",
    "    for k in range(size, 0, -1):\n",
    "        poly = np.poly1d(roots[0:k], True)\n",
    "        factor = np.multiply.reduce(prob[0:k])\n",
    "        C[k, 0:k+1] = poly.coeffs[::-1]*factor\n",
    "        for k1 in range(size + 1):\n",
    "            fk[k] += (1. + 1.) * k1 * C[k, k1]*S[k + k1]\n",
    "        for i in range(1, 2*(k-1)):\n",
    "            S[i] = (1. - prob[k-1])*S[i] + prob[k-1]*S[i+1]\n",
    "\n",
    "    return fk\n",
    "\n",
    "none_product = 50000\n",
    "\n",
    "def applyParallel(dfGrouped, func):\n",
    "    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)\n",
    "    return pd.concat(retLst)\n",
    "\n",
    "def create_products(df):\n",
    "    # print(df.product_id.values.shape)\n",
    "    products = df.product_id.values\n",
    "    prob = df.prediction.values\n",
    "\n",
    "    sort_index = np.argsort(prob)[::-1]\n",
    "\n",
    "    values = ankur_search(prob[sort_index][0:80], dtype=np.float64)\n",
    "\n",
    "    index = np.argmax(values)\n",
    "\n",
    "    print('iteration', df.shape[0], 'optimal value', index)\n",
    "\n",
    "    best = ' '.join(map(lambda x: str(x) if x != none_product else 'None', products[sort_index][0:index]))\n",
    "    df = df[0:1]\n",
    "    df.loc[:, 'products'] = best\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "check  = data[(data['order_id'] == 65432) | (data['order_id'] == 203668)]\n",
    "check  = data[data['order_id'] == 65432]\n",
    "\n",
    "\n",
    "prob = check.prediction.values\n",
    "sort_index = np.argsort(prob)[::-1]\n",
    "values = fast_search(prob[sort_index][0:80], dtype=np.float64)\n",
    "print(values)\n",
    "\n",
    "\n",
    "gp = data.groupby('order_id')['not_a_product'].apply(lambda x: np.multiply.reduce(x.values)).reset_index()\n",
    "gp.rename(columns={'not_a_product': 'prediction'}, inplace=True)\n",
    "gp['product_id'] = none_product\n",
    "\n",
    "\n",
    "data = applyParallel(data.groupby(data.order_id), create_products).reset_index()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
